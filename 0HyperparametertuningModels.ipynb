{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all preprocessed CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load listings data and review data\n",
    "listings_replication= 'replication.csv'\n",
    "listings_ordinal_encoded= 'listings_features_ordinal_encoded.csv'\n",
    "listings_ordinal_encoded_replication= 'listings_features_ordinal_encoded_replication.csv'\n",
    "reviews= 'listings_features_reviews.csv'\n",
    "textblob_sentiment= 'average_sentiment_per_listing_textblob.csv'\n",
    "vader_sentiment= 'average_sentiment_per_listing_vader.csv'\n",
    "ordinal_encoded_features = pd.read_csv(listings_ordinal_encoded)\n",
    "ordinal_encoded_replication_features = pd.read_csv(listings_ordinal_encoded_replication)\n",
    "reviews_features = pd.read_csv(reviews)\n",
    "textblob_sentiment_features= pd.read_csv(textblob_sentiment)\n",
    "vader_sentiment_features= pd.read_csv(vader_sentiment)\n",
    "replication_features= pd.read_csv(listings_replication)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Show full width of column content\n",
    "pd.set_option('display.max_rows', None)  # Show all rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape of these 4 sets of features. We see that the replication deletes listings with prices above $500. Therefore it has less rows than the label encoded and review-related features.\n",
    "As we can see those 2 have the same data cleaning approach, such that we can easily compare them together as only the features (columns) differ.\n",
    "The sentiment related features are preprocessed on the reviews and therefore have a different shape, however the download contains the same listings, listings with high prices are not removed, but for example non-english reviews are removed and therefore the number of listings is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the sentiment related sets are divided between Textblob and Vader sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication shape: (200361, 12)\n",
      "Ordinal encoded shape: (210937, 98)\n",
      "Ordinal encoded replication shape: (200361, 98)\n",
      "Reviews shape: (210937, 11)\n",
      "Textblob Sentiment shape: (270585, 6)\n",
      "Vader Sentiment shape: (270585, 6)\n"
     ]
    }
   ],
   "source": [
    "#Shapes\n",
    "print('Replication shape:', replication_features.shape)\n",
    "print('Ordinal encoded shape:', ordinal_encoded_features.shape)\n",
    "print('Ordinal encoded replication shape:', ordinal_encoded_replication_features.shape)\n",
    "print('Reviews shape:', reviews_features.shape)\n",
    "print('Textblob Sentiment shape:', textblob_sentiment_features.shape)\n",
    "print('Vader Sentiment shape:', vader_sentiment_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>number_of_reviews_l30d</th>\n",
       "      <th>review_scores_accuracy</th>\n",
       "      <th>review_scores_cleanliness</th>\n",
       "      <th>review_scores_checkin</th>\n",
       "      <th>review_scores_communication</th>\n",
       "      <th>review_scores_location</th>\n",
       "      <th>review_scores_value</th>\n",
       "      <th>days_between_first_review</th>\n",
       "      <th>days_since_last_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888038298563003925</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.88</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.88</td>\n",
       "      <td>4.75</td>\n",
       "      <td>645</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50666229</td>\n",
       "      <td>165</td>\n",
       "      <td>6</td>\n",
       "      <td>4.87</td>\n",
       "      <td>4.85</td>\n",
       "      <td>4.99</td>\n",
       "      <td>4.98</td>\n",
       "      <td>4.93</td>\n",
       "      <td>4.62</td>\n",
       "      <td>1131</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>933523</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.93</td>\n",
       "      <td>4.97</td>\n",
       "      <td>4.99</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.74</td>\n",
       "      <td>3775</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>656222143862971193</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4.86</td>\n",
       "      <td>4.86</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.93</td>\n",
       "      <td>4.71</td>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20259345</td>\n",
       "      <td>712</td>\n",
       "      <td>7</td>\n",
       "      <td>4.93</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.92</td>\n",
       "      <td>4.93</td>\n",
       "      <td>4.83</td>\n",
       "      <td>4.77</td>\n",
       "      <td>2356</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  number_of_reviews  number_of_reviews_l30d  \\\n",
       "0  888038298563003925                  8                       0   \n",
       "1            50666229                165                       6   \n",
       "2              933523                140                       2   \n",
       "3  656222143862971193                 14                       0   \n",
       "4            20259345                712                       7   \n",
       "\n",
       "   review_scores_accuracy  review_scores_cleanliness  review_scores_checkin  \\\n",
       "0                    5.00                       4.88                   4.75   \n",
       "1                    4.87                       4.85                   4.99   \n",
       "2                    4.84                       4.93                   4.97   \n",
       "3                    4.86                       4.86                   5.00   \n",
       "4                    4.93                       4.84                   4.92   \n",
       "\n",
       "   review_scores_communication  review_scores_location  review_scores_value  \\\n",
       "0                         4.50                    4.88                 4.75   \n",
       "1                         4.98                    4.93                 4.62   \n",
       "2                         4.99                    4.91                 4.74   \n",
       "3                         5.00                    4.93                 4.71   \n",
       "4                         4.93                    4.83                 4.77   \n",
       "\n",
       "   days_between_first_review  days_since_last_review  \n",
       "0                        645                     280  \n",
       "1                       1131                       7  \n",
       "2                       3775                       7  \n",
       "3                        750                      64  \n",
       "4                       2356                      13  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make sure that each id column has the same name (id instead of listing_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "textblob_sentiment_features['id']=textblob_sentiment_features['listing_id']\n",
    "vader_sentiment_features['id']=vader_sentiment_features['listing_id']\n",
    "textblob_sentiment_features.drop(columns=['listing_id'], inplace=True)\n",
    "vader_sentiment_features.drop(columns=['listing_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews sentiment shape: (201865, 16)\n",
      "Reviews sentiment shape: (201865, 16)\n"
     ]
    }
   ],
   "source": [
    "#We will merge the sentiment and reviews to get the review-related features and keep sure this are the same listings as in the non-review related feature sets.\n",
    "reviews_sentiment_features_textblob=pd.merge(textblob_sentiment_features, reviews_features, on='id', how='inner')\n",
    "print('Reviews sentiment shape:', reviews_sentiment_features_textblob.shape)\n",
    "reviews_sentiment_features_vader=pd.merge(vader_sentiment_features, reviews_features, on='id', how='inner')\n",
    "print('Reviews sentiment shape:', reviews_sentiment_features_vader.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep the same listings in all datasets except for the replication\n",
    "listings=reviews_sentiment_features_textblob['id']\n",
    "ordinal_encoded_features=ordinal_encoded_features[ordinal_encoded_features['id'].isin(listings)]\n",
    "reviews_features=reviews_features[reviews_features['id'].isin(listings)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews data do not include the price right now, we add the price to reviews_data_including price, because it can be useful later on in the research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews sentiment including price shape: (201865, 17)\n",
      "Reviews including price shape: (201865, 12)\n",
      "Reviews sentiment including price shape: (201865, 17)\n",
      "Reviews including price shape: (201865, 12)\n"
     ]
    }
   ],
   "source": [
    "#Add the price column from ordinal encoded to the reviews_sentiment_features\n",
    "reviews_sentiment_including_price_textblob=pd.merge(reviews_sentiment_features_textblob, ordinal_encoded_features[['id','price']], on='id', how='inner')\n",
    "reviews_features_including_price_textblob=pd.merge(reviews_features, ordinal_encoded_features[['id','price']], on='id', how='inner')\n",
    "print('Reviews sentiment including price shape:', reviews_sentiment_including_price_textblob.shape)\n",
    "print('Reviews including price shape:', reviews_features_including_price_textblob.shape)\n",
    "\n",
    "reviews_sentiment_including_price_vader=pd.merge(reviews_sentiment_features_vader, ordinal_encoded_features[['id','price']], on='id', how='inner')\n",
    "reviews_features_including_price_vader=pd.merge(reviews_features, ordinal_encoded_features[['id','price']], on='id', how='inner')\n",
    "print('Reviews sentiment including price shape:', reviews_sentiment_including_price_vader.shape)\n",
    "print('Reviews including price shape:', reviews_features_including_price_vader.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_without_sentiment_listings_ordinal_encoded= pd.merge(reviews_sentiment_features_textblob, ordinal_encoded_features, on='id', how='inner').drop(columns=['topic_0_average_sentiment','topic_1_average_sentiment','topic_2_average_sentiment','topic_3_average_sentiment'])\n",
    "reviews_sentiment_features_textblob_ordinal_encoded= pd.merge(reviews_sentiment_features_textblob, ordinal_encoded_features, on='id', how='inner')  \n",
    "reviews_sentiment_features_vader_ordinal_encoded= pd.merge(reviews_sentiment_features_vader, ordinal_encoded_features, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to four different feature sets, with the following shapes, divided into textblob and vader sentiment. These are the feature sets that we are using for research question 2 and research question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature set 1 replication: (200361, 98)\n",
      "Feature set 1: (201865, 98)\n",
      "Feature set 2: (201865, 109)\n",
      "Feature set 3 textblob: (201865, 113)\n",
      "Feature set 3 vader: (201865, 113)\n",
      "Feature set 4 textblob: (201865, 17)\n",
      "Feature set 4 vader: (201865, 17)\n"
     ]
    }
   ],
   "source": [
    "feature_set_1_replication=ordinal_encoded_replication_features\n",
    "feature_set_1=ordinal_encoded_features\n",
    "feature_set_2=reviews_without_sentiment_listings_ordinal_encoded\n",
    "feature_set_3_textblob=reviews_sentiment_features_textblob_ordinal_encoded\n",
    "feature_set_3_vader=reviews_sentiment_features_vader_ordinal_encoded\n",
    "feature_set_4_textblob=reviews_sentiment_including_price_textblob\n",
    "feature_set_4_vader=reviews_sentiment_including_price_vader\n",
    "\n",
    "print('Feature set 1 replication:' , feature_set_1_replication.shape)\n",
    "print('Feature set 1:' , feature_set_1.shape)\n",
    "print('Feature set 2:' , feature_set_2.shape)\n",
    "print('Feature set 3 textblob:' , feature_set_3_textblob.shape)\n",
    "print('Feature set 3 vader:' , feature_set_3_vader.shape)\n",
    "print('Feature set 4 textblob:' , feature_set_4_textblob.shape)\n",
    "print('Feature set 4 vader:' , feature_set_4_vader.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['topic_0_average_sentiment', 'topic_1_average_sentiment',\n",
       "       'topic_2_average_sentiment', 'topic_3_average_sentiment',\n",
       "       'average_word_count', 'id', 'number_of_reviews',\n",
       "       'number_of_reviews_l30d', 'review_scores_accuracy',\n",
       "       'review_scores_cleanliness', 'review_scores_checkin',\n",
       "       'review_scores_communication', 'review_scores_location',\n",
       "       'review_scores_value', 'days_between_first_review',\n",
       "       'days_since_last_review', 'price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set_4_vader.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['average_word_count', 'id', 'number_of_reviews',\n",
       "       'number_of_reviews_l30d', 'review_scores_accuracy',\n",
       "       'review_scores_cleanliness', 'review_scores_checkin',\n",
       "       'review_scores_communication', 'review_scores_location',\n",
       "       'review_scores_value', 'days_between_first_review',\n",
       "       'days_since_last_review', 'neighborhood_overview', 'host_location',\n",
       "       'host_about', 'host_response_time', 'host_response_rate',\n",
       "       'host_acceptance_rate', 'host_is_superhost'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set_2.columns[0:19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same train-test split for those four feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Set 1 - Train Shape: (181678, 98)\n",
      "Feature Set 1 - Test Shape: (20187, 98)\n",
      "Feature Set 2 - Train Shape: (181678, 109)\n",
      "Feature Set 2 - Test Shape: (20187, 109)\n",
      "Feature Set 3 (TextBlob) - Train Shape: (181678, 113)\n",
      "Feature Set 3 (TextBlob) - Test Shape: (20187, 113)\n",
      "Feature Set 3 (Vader) - Train Shape: (181678, 113)\n",
      "Feature Set 3 (Vader) - Test Shape: (20187, 113)\n",
      "Feature Set 4 (TextBlob) - Train Shape: (181678, 17)\n",
      "Feature Set 4 (TextBlob) - Test Shape: (20187, 17)\n",
      "Feature Set 4 (Vader) - Train Shape: (181678, 17)\n",
      "Feature Set 4 (Vader) - Test Shape: (20187, 17)\n"
     ]
    }
   ],
   "source": [
    "# Ensure all feature sets are aligned on the 'id' column\n",
    "# Use inner join on the 'id' column to ensure consistency across feature sets\n",
    "feature_set_2 = feature_set_2.merge(feature_set_1[['id']], on='id', how='inner')\n",
    "feature_set_3_textblob = feature_set_3_textblob.merge(feature_set_1[['id']], on='id', how='inner')\n",
    "feature_set_3_vader = feature_set_3_vader.merge(feature_set_1[['id']], on='id', how='inner')\n",
    "feature_set_4_textblob = feature_set_4_textblob.merge(feature_set_1[['id']], on='id', how='inner')\n",
    "feature_set_4_vader = feature_set_4_vader.merge(feature_set_1[['id']], on='id', how='inner')\n",
    "\n",
    "# Extract unique IDs\n",
    "unique_ids = feature_set_1['id'].unique()\n",
    "\n",
    "# Perform a 90/10 train/test split\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.1, random_state=42)\n",
    "\n",
    "# Define masks for train and test sets\n",
    "train_mask = feature_set_1['id'].isin(train_ids)\n",
    "test_mask = feature_set_1['id'].isin(test_ids)\n",
    "\n",
    "# Apply the split to each feature set\n",
    "feature_set_1_train = feature_set_1[train_mask]\n",
    "feature_set_1_test = feature_set_1[test_mask]\n",
    "\n",
    "feature_set_2_train = feature_set_2[feature_set_2['id'].isin(train_ids)]\n",
    "feature_set_2_test = feature_set_2[feature_set_2['id'].isin(test_ids)]\n",
    "\n",
    "feature_set_3_textblob_train = feature_set_3_textblob[feature_set_3_textblob['id'].isin(train_ids)]\n",
    "feature_set_3_textblob_test = feature_set_3_textblob[feature_set_3_textblob['id'].isin(test_ids)]\n",
    "\n",
    "feature_set_3_vader_train = feature_set_3_vader[feature_set_3_vader['id'].isin(train_ids)]\n",
    "feature_set_3_vader_test = feature_set_3_vader[feature_set_3_vader['id'].isin(test_ids)]\n",
    "\n",
    "feature_set_4_textblob_train = feature_set_4_textblob[feature_set_4_textblob['id'].isin(train_ids)]\n",
    "feature_set_4_textblob_test = feature_set_4_textblob[feature_set_4_textblob['id'].isin(test_ids)]\n",
    "\n",
    "feature_set_4_vader_train = feature_set_4_vader[feature_set_4_vader['id'].isin(train_ids)]\n",
    "feature_set_4_vader_test = feature_set_4_vader[feature_set_4_vader['id'].isin(test_ids)]\n",
    "\n",
    "# Checking shapes and consistency\n",
    "print(\"Feature Set 1 - Train Shape:\", feature_set_1_train.shape)\n",
    "print(\"Feature Set 1 - Test Shape:\", feature_set_1_test.shape)\n",
    "\n",
    "print(\"Feature Set 2 - Train Shape:\", feature_set_2_train.shape)\n",
    "print(\"Feature Set 2 - Test Shape:\", feature_set_2_test.shape)\n",
    "\n",
    "print(\"Feature Set 3 (TextBlob) - Train Shape:\", feature_set_3_textblob_train.shape)\n",
    "print(\"Feature Set 3 (TextBlob) - Test Shape:\", feature_set_3_textblob_test.shape)\n",
    "\n",
    "print(\"Feature Set 3 (Vader) - Train Shape:\", feature_set_3_vader_train.shape)\n",
    "print(\"Feature Set 3 (Vader) - Test Shape:\", feature_set_3_vader_test.shape)\n",
    "\n",
    "print(\"Feature Set 4 (TextBlob) - Train Shape:\", feature_set_4_textblob_train.shape)\n",
    "print(\"Feature Set 4 (TextBlob) - Test Shape:\", feature_set_4_textblob_test.shape)\n",
    "\n",
    "print(\"Feature Set 4 (Vader) - Train Shape:\", feature_set_4_vader_train.shape)\n",
    "print(\"Feature Set 4 (Vader) - Test Shape:\", feature_set_4_vader_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete ID column from all feature sets (Train and Test), because it should not have a meaning\n",
    "# Delete the 'id' column from all train and test sets\n",
    "feature_set_1_train = feature_set_1_train.drop(columns=['id'])\n",
    "feature_set_1_test = feature_set_1_test.drop(columns=['id'])\n",
    "\n",
    "feature_set_2_train = feature_set_2_train.drop(columns=['id'])\n",
    "feature_set_2_test = feature_set_2_test.drop(columns=['id'])\n",
    "\n",
    "feature_set_3_textblob_train = feature_set_3_textblob_train.drop(columns=['id'])\n",
    "feature_set_3_textblob_test = feature_set_3_textblob_test.drop(columns=['id'])\n",
    "\n",
    "feature_set_3_vader_train = feature_set_3_vader_train.drop(columns=['id'])\n",
    "feature_set_3_vader_test = feature_set_3_vader_test.drop(columns=['id'])\n",
    "\n",
    "feature_set_4_textblob_train = feature_set_4_textblob_train.drop(columns=['id'])\n",
    "feature_set_4_textblob_test = feature_set_4_textblob_test.drop(columns=['id'])\n",
    "\n",
    "feature_set_4_vader_train = feature_set_4_vader_train.drop(columns=['id'])\n",
    "feature_set_4_vader_test = feature_set_4_vader_test.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Set 1 - Train Shape: (181678, 97)\n",
      "Feature Set 1 - Test Shape: (20187, 97)\n",
      "Feature Set 2 - Train Shape: (181678, 108)\n",
      "Feature Set 2 - Test Shape: (20187, 108)\n",
      "Feature Set 3 (TextBlob) - Train Shape: (181678, 112)\n",
      "Feature Set 3 (TextBlob) - Test Shape: (20187, 112)\n",
      "Feature Set 3 (Vader) - Train Shape: (181678, 112)\n",
      "Feature Set 3 (Vader) - Test Shape: (20187, 112)\n",
      "Feature Set 4 (TextBlob) - Train Shape: (181678, 16)\n",
      "Feature Set 4 (TextBlob) - Test Shape: (20187, 16)\n",
      "Feature Set 4 (Vader) - Train Shape: (181678, 16)\n",
      "Feature Set 4 (Vader) - Test Shape: (20187, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Set 1 - Train Shape:\", feature_set_1_train.shape)\n",
    "print(\"Feature Set 1 - Test Shape:\", feature_set_1_test.shape)\n",
    "\n",
    "print(\"Feature Set 2 - Train Shape:\", feature_set_2_train.shape)\n",
    "print(\"Feature Set 2 - Test Shape:\", feature_set_2_test.shape)\n",
    "\n",
    "print(\"Feature Set 3 (TextBlob) - Train Shape:\", feature_set_3_textblob_train.shape)\n",
    "print(\"Feature Set 3 (TextBlob) - Test Shape:\", feature_set_3_textblob_test.shape)\n",
    "\n",
    "print(\"Feature Set 3 (Vader) - Train Shape:\", feature_set_3_vader_train.shape)\n",
    "print(\"Feature Set 3 (Vader) - Test Shape:\", feature_set_3_vader_test.shape)\n",
    "\n",
    "print(\"Feature Set 4 (TextBlob) - Train Shape:\", feature_set_4_textblob_train.shape)\n",
    "print(\"Feature Set 4 (TextBlob) - Test Shape:\", feature_set_4_textblob_test.shape)\n",
    "\n",
    "print(\"Feature Set 4 (Vader) - Train Shape:\", feature_set_4_vader_train.shape)\n",
    "print(\"Feature Set 4 (Vader) - Test Shape:\", feature_set_4_vader_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all those train and test sets to CSV files\n",
    "feature_set_1_train.to_csv('feature_set_1_train.csv', index=False)\n",
    "feature_set_1_test.to_csv('feature_set_1_test.csv', index=False)\n",
    "\n",
    "feature_set_2_train.to_csv('feature_set_2_train.csv', index=False)\n",
    "feature_set_2_test.to_csv('feature_set_2_test.csv', index=False)\n",
    "\n",
    "feature_set_3_textblob_train.to_csv('feature_set_3_textblob_train.csv', index=False)\n",
    "feature_set_3_textblob_test.to_csv('feature_set_3_textblob_test.csv', index=False)\n",
    "\n",
    "feature_set_3_vader_train.to_csv('feature_set_3_vader_train.csv', index=False)\n",
    "feature_set_3_vader_test.to_csv('feature_set_3_vader_test.csv', index=False)\n",
    "\n",
    "feature_set_4_textblob_train.to_csv('feature_set_4_textblob_train.csv', index=False)\n",
    "feature_set_4_textblob_test.to_csv('feature_set_4_textblob_test.csv', index=False)\n",
    "\n",
    "feature_set_4_vader_train.to_csv('feature_set_4_vader_train.csv', index=False)\n",
    "feature_set_4_vader_test.to_csv('feature_set_4_vader_test.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In purpose of research question 1, we start with the hyperparameter tuning of the XGboost model in the replication, and compare the results with the mean-price prediction model (Baseline), Linear regression and XGBoost with the default parameters. This leads to the following set of best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison with (Peng, 2020) we also keep a separate test set for replication purpose, both for linear regression as well for the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (Mean Prediction):\n",
      "RMSE: 0.597, R-squared: 0.000\n",
      "\n",
      "Linear Regression (5-fold CV):\n",
      "Mean RMSE (CV): 0.456, Mean R-squared (CV): 0.415\n",
      "\n",
      "Linear Regression on Test Set:\n",
      "RMSE: 0.457, R-squared: 0.420\n",
      "\n",
      "XGBoost (default parameters, 5-fold CV):\n",
      "Mean RMSE (CV): 0.415, Mean R-squared (CV): 0.516\n",
      "\n",
      "XGBoost (default parameters) on Test Set:\n",
      "RMSE: 0.416, R-squared: 0.520\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "\n",
      "XGBoost (tuned parameters, 5-fold CV):\n",
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 12, 'n_estimators': 1200, 'subsample': 0.8}\n",
      "Mean RMSE (CV): 0.407, Mean R-squared (CV): 0.535\n",
      "\n",
      "XGBoost (tuned parameters) on Test Set:\n",
      "RMSE: 0.407, R-squared: 0.540\n"
     ]
    }
   ],
   "source": [
    "# Ensure reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "features = replication_features.drop(columns=['price'])\n",
    "target = replication_features['price']\n",
    "\n",
    "# Train-test split (90% train, 10% test)\n",
    "X_train_replication, X_test_replication, y_train_replication, y_test_replication = train_test_split(\n",
    "    features, target, test_size=0.10, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Scale training data only\n",
    "scaler = StandardScaler()\n",
    "X_train_replication_scaled = scaler.fit_transform(X_train_replication)\n",
    "y_train_replication = y_train_replication.reset_index(drop=True)\n",
    "\n",
    "# Scale test data\n",
    "X_test_replication_scaled = scaler.transform(X_test_replication)\n",
    "\n",
    "# Baseline 1: Mean Prediction\n",
    "baseline_mean_price = y_train_replication.mean()\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_train_replication, [baseline_mean_price] * len(y_train_replication)))\n",
    "baseline_r2 = r2_score(y_train_replication, [baseline_mean_price] * len(y_train_replication))\n",
    "\n",
    "print(\"Baseline (Mean Prediction):\")\n",
    "print(f\"RMSE: {baseline_rmse:.3f}, R-squared: {baseline_r2:.3f}\")\n",
    "\n",
    "# Function to evaluate a model with cross-validation\n",
    "def evaluate_model(model, X, y, cv):\n",
    "    rmse_scores, r2_scores = [], []\n",
    "    for train_idx, val_idx in cv.split(X):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_val, y_pred_val)))\n",
    "        r2_scores.append(r2_score(y_val, y_pred_val))\n",
    "    return np.mean(rmse_scores), np.mean(r2_scores)\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Linear Regression cross-validation evaluation\n",
    "linear_model = LinearRegression()\n",
    "rmse_lr_cv, r2_lr_cv = evaluate_model(linear_model, X_train_replication_scaled, y_train_replication, kf)\n",
    "\n",
    "print(\"\\nLinear Regression (5-fold CV):\")\n",
    "print(f\"Mean RMSE (CV): {rmse_lr_cv:.3f}, Mean R-squared (CV): {r2_lr_cv:.3f}\")\n",
    "\n",
    "# Train Linear Regression on full training data\n",
    "linear_model.fit(X_train_replication_scaled, y_train_replication)\n",
    "\n",
    "# Evaluate Linear Regression on test data\n",
    "y_pred_test_lr = linear_model.predict(X_test_replication_scaled)\n",
    "rmse_test_lr = np.sqrt(mean_squared_error(y_test_replication, y_pred_test_lr))\n",
    "r2_test_lr = r2_score(y_test_replication, y_pred_test_lr)\n",
    "\n",
    "print(\"\\nLinear Regression on Test Set:\")\n",
    "print(f\"RMSE: {rmse_test_lr:.3f}, R-squared: {r2_test_lr:.3f}\")\n",
    "\n",
    "# XGBoost with default parameters cross-validation evaluation\n",
    "xgb_default = XGBRegressor(random_state=RANDOM_SEED)\n",
    "rmse_xgb_cv, r2_xgb_cv = evaluate_model(xgb_default, X_train_replication_scaled, y_train_replication, kf)\n",
    "\n",
    "print(\"\\nXGBoost (default parameters, 5-fold CV):\")\n",
    "print(f\"Mean RMSE (CV): {rmse_xgb_cv:.3f}, Mean R-squared (CV): {r2_xgb_cv:.3f}\")\n",
    "\n",
    "# Train XGBoost with default parameters on full training data\n",
    "xgb_default.fit(X_train_replication_scaled, y_train_replication)\n",
    "\n",
    "# Evaluate XGBoost on test data\n",
    "y_pred_test_xgb = xgb_default.predict(X_test_replication_scaled)\n",
    "rmse_test_xgb = np.sqrt(mean_squared_error(y_test_replication, y_pred_test_xgb))\n",
    "r2_test_xgb = r2_score(y_test_replication, y_pred_test_xgb)\n",
    "\n",
    "print(\"\\nXGBoost (default parameters) on Test Set:\")\n",
    "print(f\"RMSE: {rmse_test_xgb:.3f}, R-squared: {r2_test_xgb:.3f}\")\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'max_depth': [8, 10, 12],            # Moderate tree depths\n",
    "    'n_estimators': [400, 800, 1200],  # Number of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.3],      # Step size shrinkage\n",
    "    'subsample': [0.8, 1.0],                # Row sampling\n",
    "}\n",
    "\n",
    "\n",
    "# GridSearchCV setup\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    estimator=XGBRegressor(random_state=RANDOM_SEED),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=kf,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on the training data\n",
    "xgb_grid_search.fit(X_train_replication_scaled, y_train_replication)\n",
    "\n",
    "# Cross-validation evaluation for the best XGBoost model\n",
    "best_xgb = xgb_grid_search.best_estimator_\n",
    "rmse_best_xgb_cv, r2_best_xgb_cv = evaluate_model(best_xgb, X_train_replication_scaled, y_train_replication, kf)\n",
    "\n",
    "print(\"\\nXGBoost (tuned parameters, 5-fold CV):\")\n",
    "print(f\"Best Parameters: {xgb_grid_search.best_params_}\")\n",
    "print(f\"Mean RMSE (CV): {rmse_best_xgb_cv:.3f}, Mean R-squared (CV): {r2_best_xgb_cv:.3f}\")\n",
    "\n",
    "# Train the best XGBoost model on full training data\n",
    "best_xgb.fit(X_train_replication_scaled, y_train_replication)\n",
    "\n",
    "# Evaluate the best XGBoost model on test data\n",
    "y_pred_test_best_xgb = best_xgb.predict(X_test_replication_scaled)\n",
    "rmse_test_best_xgb = np.sqrt(mean_squared_error(y_test_replication, y_pred_test_best_xgb))\n",
    "r2_test_best_xgb = r2_score(y_test_replication, y_pred_test_best_xgb)\n",
    "\n",
    "print(\"\\nXGBoost (tuned parameters) on Test Set:\")\n",
    "print(f\"RMSE: {rmse_test_best_xgb:.3f}, R-squared: {r2_test_best_xgb:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same with feature set 1, according to the replication preprocessing approach. This means that all listings with a price above 500 are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (Mean Prediction):\n",
      "RMSE: 0.597, R-squared: 0.000\n",
      "\n",
      "Linear Regression (5-fold CV):\n",
      "Mean RMSE (CV): 0.412, Mean R-squared (CV): 0.522\n",
      "\n",
      "Linear Regression on Test Set:\n",
      "RMSE: 0.413, R-squared: 0.528\n",
      "\n",
      "XGBoost (default parameters, 5-fold CV):\n",
      "Mean RMSE (CV): 0.300, Mean R-squared (CV): 0.747\n",
      "\n",
      "XGBoost (default parameters) on Test Set:\n",
      "RMSE: 0.299, R-squared: 0.752\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "\n",
      "XGBoost (tuned parameters, 5-fold CV):\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 1200, 'subsample': 1.0}\n",
      "Mean RMSE (CV): 0.278, Mean R-squared (CV): 0.783\n",
      "\n",
      "XGBoost (tuned parameters) on Test Set:\n",
      "RMSE: 0.274, R-squared: 0.792\n"
     ]
    }
   ],
   "source": [
    "# Ensure reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "features_fs1 = feature_set_1_replication.drop(columns=['price'])\n",
    "target_fs1 = feature_set_1_replication['price']\n",
    "\n",
    "# Train-test split (90% train, 10% test)\n",
    "X_train_fs1, X_test_fs1, y_train_fs1, y_test_fs1 = train_test_split(\n",
    "    features_fs1, target_fs1, test_size=0.10, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Scale training data only\n",
    "scaler_fs1 = StandardScaler()\n",
    "X_train_fs1_scaled = scaler_fs1.fit_transform(X_train_fs1)\n",
    "X_train_fs1_scaled = X_train_fs1_scaled.astype('float32')\n",
    "y_train_fs1 = y_train_fs1.reset_index(drop=True)\n",
    "\n",
    "# Scale test data\n",
    "X_test_fs1_scaled = scaler_fs1.transform(X_test_fs1)\n",
    "X_test_fs1_scaled = X_test_fs1_scaled.astype('float32')\n",
    "\n",
    "# Baseline 1: Mean Prediction\n",
    "baseline_mean_price_fs1 = y_train_fs1.mean()\n",
    "baseline_rmse_fs1 = np.sqrt(mean_squared_error(y_train_fs1, [baseline_mean_price_fs1] * len(y_train_fs1)))\n",
    "baseline_r2_fs1 = r2_score(y_train_fs1, [baseline_mean_price_fs1] * len(y_train_fs1))\n",
    "\n",
    "print(\"Baseline (Mean Prediction):\")\n",
    "print(f\"RMSE: {baseline_rmse_fs1:.3f}, R-squared: {baseline_r2_fs1:.3f}\")\n",
    "\n",
    "# Function to evaluate a model with cross-validation\n",
    "def evaluate_model_fs1(model, X, y, cv):\n",
    "    rmse_scores, r2_scores = [], []\n",
    "    for train_idx, val_idx in cv.split(X):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_val, y_pred_val)))\n",
    "        r2_scores.append(r2_score(y_val, y_pred_val))\n",
    "    return np.mean(rmse_scores), np.mean(r2_scores)\n",
    "\n",
    "# Cross-validation setup\n",
    "kf_fs1 = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Linear Regression cross-validation evaluation\n",
    "linear_model_fs1 = LinearRegression()\n",
    "rmse_lr_fs1_cv, r2_lr_fs1_cv = evaluate_model_fs1(linear_model_fs1, X_train_fs1_scaled, y_train_fs1, kf_fs1)\n",
    "\n",
    "print(\"\\nLinear Regression (5-fold CV):\")\n",
    "print(f\"Mean RMSE (CV): {rmse_lr_fs1_cv:.3f}, Mean R-squared (CV): {r2_lr_fs1_cv:.3f}\")\n",
    "\n",
    "# Train Linear Regression on full training data\n",
    "linear_model_fs1.fit(X_train_fs1_scaled, y_train_fs1)\n",
    "\n",
    "# Evaluate Linear Regression on test data\n",
    "y_pred_test_lr_fs1 = linear_model_fs1.predict(X_test_fs1_scaled)\n",
    "rmse_test_lr_fs1 = np.sqrt(mean_squared_error(y_test_fs1, y_pred_test_lr_fs1))\n",
    "r2_test_lr_fs1 = r2_score(y_test_fs1, y_pred_test_lr_fs1)\n",
    "\n",
    "print(\"\\nLinear Regression on Test Set:\")\n",
    "print(f\"RMSE: {rmse_test_lr_fs1:.3f}, R-squared: {r2_test_lr_fs1:.3f}\")\n",
    "\n",
    "# XGBoost with default parameters cross-validation evaluation\n",
    "xgb_default_fs1 = XGBRegressor(random_state=RANDOM_SEED)\n",
    "rmse_xgb_default_fs1_cv, r2_xgb_default_fs1_cv = evaluate_model_fs1(xgb_default_fs1, X_train_fs1_scaled, y_train_fs1, kf_fs1)\n",
    "\n",
    "print(\"\\nXGBoost (default parameters, 5-fold CV):\")\n",
    "print(f\"Mean RMSE (CV): {rmse_xgb_default_fs1_cv:.3f}, Mean R-squared (CV): {r2_xgb_default_fs1_cv:.3f}\")\n",
    "\n",
    "# Train XGBoost with default parameters on full training data\n",
    "xgb_default_fs1.fit(X_train_fs1_scaled, y_train_fs1)\n",
    "\n",
    "# Evaluate XGBoost on test data\n",
    "y_pred_test_xgb_fs1 = xgb_default_fs1.predict(X_test_fs1_scaled)\n",
    "rmse_test_xgb_fs1 = np.sqrt(mean_squared_error(y_test_fs1, y_pred_test_xgb_fs1))\n",
    "r2_test_xgb_fs1 = r2_score(y_test_fs1, y_pred_test_xgb_fs1)\n",
    "\n",
    "print(\"\\nXGBoost (default parameters) on Test Set:\")\n",
    "print(f\"RMSE: {rmse_test_xgb_fs1:.3f}, R-squared: {r2_test_xgb_fs1:.3f}\")\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid_fs1 = {\n",
    "    'max_depth': [10, 12],\n",
    "    'n_estimators': [800, 1200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# GridSearchCV setup\n",
    "xgb_grid_search_fs1 = GridSearchCV(\n",
    "    estimator=XGBRegressor(random_state=RANDOM_SEED),\n",
    "    param_grid=param_grid_fs1,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=kf_fs1,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on the training data\n",
    "xgb_grid_search_fs1.fit(X_train_fs1_scaled, y_train_fs1)\n",
    "\n",
    "# Cross-validation evaluation for the best XGBoost model\n",
    "best_xgb_fs1 = xgb_grid_search_fs1.best_estimator_\n",
    "rmse_xgb_tuned_fs1_cv, r2_xgb_tuned_fs1_cv = evaluate_model_fs1(best_xgb_fs1, X_train_fs1_scaled, y_train_fs1, kf_fs1)\n",
    "\n",
    "print(\"\\nXGBoost (tuned parameters, 5-fold CV):\")\n",
    "print(f\"Best Parameters: {xgb_grid_search_fs1.best_params_}\")\n",
    "print(f\"Mean RMSE (CV): {rmse_xgb_tuned_fs1_cv:.3f}, Mean R-squared (CV): {r2_xgb_tuned_fs1_cv:.3f}\")\n",
    "\n",
    "# Train the best XGBoost model on full training data\n",
    "best_xgb_fs1.fit(X_train_fs1_scaled, y_train_fs1)\n",
    "\n",
    "# Evaluate the best XGBoost model on test data\n",
    "y_pred_test_best_xgb_fs1 = best_xgb_fs1.predict(X_test_fs1_scaled)\n",
    "rmse_test_best_xgb_fs1 = np.sqrt(mean_squared_error(y_test_fs1, y_pred_test_best_xgb_fs1))\n",
    "r2_test_best_xgb_fs1 = r2_score(y_test_fs1, y_pred_test_best_xgb_fs1)\n",
    "\n",
    "print(\"\\nXGBoost (tuned parameters) on Test Set:\")\n",
    "print(f\"RMSE: {rmse_test_best_xgb_fs1:.3f}, R-squared: {r2_test_best_xgb_fs1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results on test set\n",
    "# Scale the test set using the same scaler fitted on the training data\n",
    "X_test_fs1_scaled = scaler_fs1.transform(X_test_fs1)\n",
    "# Evaluate Linear Regression on the Test Set\n",
    "y_pred_test_lr_fs1 = linear_model_fs1.predict(X_test_fs1_scaled)\n",
    "rmse_test_lr_fs1 = np.sqrt(mean_squared_error(y_test_fs1, y_pred_test_lr_fs1))\n",
    "r2_test_lr_fs1 = r2_score(y_test_fs1, y_pred_test_lr_fs1)\n",
    "\n",
    "print(\"\\nLinear Regression:\")\n",
    "print(f\"Test RMSE: {rmse_test_lr_fs1:.3f}\")\n",
    "print(f\"Test R-squared: {r2_test_lr_fs1:.3f}\")\n",
    "\n",
    "# Best model retrieved from GridSearchCV\n",
    "best_xgb_fs1 = xgb_grid_search_fs1.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_test_fs1 = best_xgb_fs1.predict(X_test_fs1_scaled)   \n",
    "\n",
    "# Evaluate RMSE and R-squared on the test set\n",
    "rmse_test_fs1 = np.sqrt(mean_squared_error(y_test_fs1, y_pred_test_fs1))\n",
    "r2_test_fs1 = r2_score(y_test_fs1, y_pred_test_fs1)\n",
    "\n",
    "# Print test set evaluation metrics\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "print(f\"RMSE: {rmse_test_fs1:.3f}\")\n",
    "print(f\"R-squared: {r2_test_fs1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model will generate the same results for all 4 feature sets, because they all consist of the same listings in the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the four feature sets to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning for feature set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (Mean Prediction):\n",
      "RMSE: 0.698, R-squared: 0.000\n",
      "\n",
      "Linear Regression:\n",
      "Mean RMSE (CV): 0.464, Mean R-squared (CV): 0.557\n",
      "\n",
      "XGBoost (default parameters):\n",
      "Mean RMSE (CV): 0.338, Mean R-squared (CV): 0.765\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      "XGBoost (tuned parameters):\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 1200, 'subsample': 1.0}\n",
      "Mean RMSE (CV): 0.313, Mean R-squared (CV): 0.799\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate a model with cross-validation\n",
    "def evaluate_model(model, X, y, cv):\n",
    "    rmse_scores, r2_scores = [], []\n",
    "    for train_idx, val_idx in cv.split(X):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_val, y_pred_val)))\n",
    "        r2_scores.append(r2_score(y_val, y_pred_val))\n",
    "    return np.mean(rmse_scores), np.mean(r2_scores)\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Scale training data only\n",
    "scaler = StandardScaler()\n",
    "features_train_1 = scaler.fit_transform(feature_set_1_train.drop(columns=['price']))\n",
    "target_train_1 = feature_set_1_train['price']\n",
    "\n",
    "# Baseline 1: Mean Prediction\n",
    "baseline_mean_price = target_train_1.mean()\n",
    "baseline_rmse = np.sqrt(mean_squared_error(target_train_1, [baseline_mean_price] * len(target_train_1)))\n",
    "baseline_r2 = r2_score(target_train_1, [baseline_mean_price] * len(target_train_1))\n",
    "\n",
    "print(\"Baseline (Mean Prediction):\")\n",
    "print(f\"RMSE: {baseline_rmse:.3f}, R-squared: {baseline_r2:.3f}\")\n",
    "\n",
    "# Baseline 2: Linear Regression\n",
    "linear_model = LinearRegression()\n",
    "rmse_lr, r2_lr = evaluate_model(linear_model, features_train_1, target_train_1, kf)\n",
    "\n",
    "print(\"\\nLinear Regression:\")\n",
    "print(f\"Mean RMSE (CV): {rmse_lr:.3f}, Mean R-squared (CV): {r2_lr:.3f}\")\n",
    "\n",
    "# XGBoost with default parameters\n",
    "xgb_default = XGBRegressor(random_state=42)\n",
    "rmse_xgb_default, r2_xgb_default = evaluate_model(xgb_default, features_train_1, target_train_1, kf)\n",
    "\n",
    "print(\"\\nXGBoost (default parameters):\")\n",
    "print(f\"Mean RMSE (CV): {rmse_xgb_default:.3f}, Mean R-squared (CV): {r2_xgb_default:.3f}\")\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'max_depth': [6, 8, 10, 12],            # Moderate tree depths\n",
    "    'n_estimators': [200, 400, 800, 1200],      # Number of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.3],    # Step size shrinkage\n",
    "    'subsample': [0.8, 1.0],              # Row sampling\n",
    "}\n",
    "\n",
    "# GridSearchCV setup\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=kf,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on the training data\n",
    "xgb_grid_search.fit(features_train_1, target_train_1)\n",
    "\n",
    "# Retrieve the best estimator and evaluate it using cross-validation\n",
    "best_xgb = xgb_grid_search.best_estimator_\n",
    "rmse_xgb_tuned, r2_xgb_tuned = evaluate_model(best_xgb, features_train_1, target_train_1, kf)\n",
    "\n",
    "print(\"\\nXGBoost (tuned parameters):\")\n",
    "print(f\"Best Parameters: {xgb_grid_search.best_params_}\")\n",
    "print(f\"Mean RMSE (CV): {rmse_xgb_tuned:.3f}, Mean R-squared (CV): {r2_xgb_tuned:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning for feature set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline (Mean Prediction) - Feature Set 2:\n",
      "RMSE: 0.698, R-squared: 0.000\n",
      "\n",
      "Linear Regression - Feature Set 2:\n",
      "Mean RMSE (CV): 0.451, Mean R-squared (CV): 0.583\n",
      "\n",
      "XGBoost (default parameters) - Feature Set 2:\n",
      "Mean RMSE (CV): 0.329, Mean R-squared (CV): 0.778\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      "XGBoost (tuned parameters) - Feature Set 2:\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 1200, 'subsample': 1.0}\n",
      "Mean RMSE (CV): 0.306, Mean R-squared (CV): 0.808\n"
     ]
    }
   ],
   "source": [
    "# Scale training data for feature set 2\n",
    "features_train_2 = scaler.fit_transform(feature_set_2_train.drop(columns=['price']))\n",
    "target_train_2 = feature_set_2_train['price']\n",
    "\n",
    "# Baseline 1: Mean Prediction\n",
    "baseline_mean_price_2 = target_train_2.mean()\n",
    "baseline_rmse_2 = np.sqrt(mean_squared_error(target_train_2, [baseline_mean_price_2] * len(target_train_2)))\n",
    "baseline_r2_2 = r2_score(target_train_2, [baseline_mean_price_2] * len(target_train_2))\n",
    "\n",
    "print(\"\\nBaseline (Mean Prediction) - Feature Set 2:\")\n",
    "print(f\"RMSE: {baseline_rmse_2:.3f}, R-squared: {baseline_r2_2:.3f}\")\n",
    "\n",
    "# Baseline 2: Linear Regression\n",
    "linear_model_2 = LinearRegression()\n",
    "rmse_lr_2, r2_lr_2 = evaluate_model(linear_model_2, features_train_2, target_train_2, kf)\n",
    "\n",
    "print(\"\\nLinear Regression - Feature Set 2:\")\n",
    "print(f\"Mean RMSE (CV): {rmse_lr_2:.3f}, Mean R-squared (CV): {r2_lr_2:.3f}\")\n",
    "\n",
    "# XGBoost with default parameters\n",
    "xgb_default_2 = XGBRegressor(random_state=42)\n",
    "rmse_xgb_default_2, r2_xgb_default_2 = evaluate_model(xgb_default_2, features_train_2, target_train_2, kf)\n",
    "\n",
    "print(\"\\nXGBoost (default parameters) - Feature Set 2:\")\n",
    "print(f\"Mean RMSE (CV): {rmse_xgb_default_2:.3f}, Mean R-squared (CV): {r2_xgb_default_2:.3f}\")\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid_2 = {\n",
    "    'max_depth': [6, 8, 10, 12],            # Moderate tree depths\n",
    "    'n_estimators': [200, 400, 800, 1200],      # Number of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.3],    # Step size shrinkage\n",
    "    'subsample': [0.8, 1.0],              # Row sampling\n",
    "}\n",
    "\n",
    "# GridSearchCV setup\n",
    "xgb_grid_search_2 = GridSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42),\n",
    "    param_grid=param_grid_2,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=kf,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on the training data for feature set 2\n",
    "xgb_grid_search_2.fit(features_train_2, target_train_2)\n",
    "\n",
    "# Retrieve the best estimator and evaluate it using cross-validation\n",
    "best_xgb_2 = xgb_grid_search_2.best_estimator_\n",
    "rmse_xgb_tuned_2, r2_xgb_tuned_2 = evaluate_model(best_xgb_2, features_train_2, target_train_2, kf)\n",
    "\n",
    "print(\"\\nXGBoost (tuned parameters) - Feature Set 2:\")\n",
    "print(f\"Best Parameters: {xgb_grid_search_2.best_params_}\")\n",
    "print(f\"Mean RMSE (CV): {rmse_xgb_tuned_2:.3f}, Mean R-squared (CV): {r2_xgb_tuned_2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparametertuning for feature set 3 comparing Textblob and Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Set 3 (TextBlob) ===\n",
      "Baseline (Mean Prediction):\n",
      "RMSE: 0.698, R-squared: 0.000\n",
      "\n",
      "Linear Regression:\n",
      "Mean RMSE (CV): 0.450, Mean R-squared (CV): 0.585\n",
      "\n",
      "XGBoost (default parameters):\n",
      "Mean RMSE (CV): 0.331, Mean R-squared (CV): 0.776\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "6 fits failed out of a total of 480.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1108, in fit\n",
      "    self._Booster = train(\n",
      "                    ~~~~~^\n",
      "        params,\n",
      "        ^^^^^^^\n",
      "    ...<9 lines>...\n",
      "        callbacks=self.callbacks,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py\", line 192, in train\n",
      "    return bst.copy()\n",
      "           ~~~~~~~~^^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 1931, in copy\n",
      "    return copy.copy(self)\n",
      "           ~~~~~~~~~^^^^^^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\copy.py\", line 80, in copy\n",
      "    return copier(x)\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 1917, in __copy__\n",
      "    return self.__deepcopy__(None)\n",
      "           ~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 1921, in __deepcopy__\n",
      "    return Booster(model_file=self)\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 1716, in __init__\n",
      "    state = model_file.__getstate__()\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 1807, in __getstate__\n",
      "    _check_call(\n",
      "    ~~~~~~~~~~~^\n",
      "        _LIB.XGBoosterSerializeToBuffer(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "            self.handle, ctypes.byref(length), ctypes.byref(cptr)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        )\n",
      "        ^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: bad allocation\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1108, in fit\n",
      "    self._Booster = train(\n",
      "                    ~~~~~^\n",
      "        params,\n",
      "        ^^^^^^^\n",
      "    ...<9 lines>...\n",
      "        callbacks=self.callbacks,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "    ~~~~~~~~~~~^\n",
      "        _LIB.XGBoosterUpdateOneIter(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "            self.handle, ctypes.c_int(iteration), dtrain.handle\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        )\n",
      "        ^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [11:43:44] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\common\\io.h:320: bad_malloc: Failed to allocate 247424336 bytes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1108, in fit\n",
      "    self._Booster = train(\n",
      "                    ~~~~~^\n",
      "        params,\n",
      "        ^^^^^^^\n",
      "    ...<9 lines>...\n",
      "        callbacks=self.callbacks,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "    ~~~~~~~~~~~^\n",
      "        _LIB.XGBoosterUpdateOneIter(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "            self.handle, ctypes.c_int(iteration), dtrain.handle\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        )\n",
      "        ^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [11:43:50] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\common\\io.h:320: bad_malloc: Failed to allocate 245624096 bytes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1108, in fit\n",
      "    self._Booster = train(\n",
      "                    ~~~~~^\n",
      "        params,\n",
      "        ^^^^^^^\n",
      "    ...<9 lines>...\n",
      "        callbacks=self.callbacks,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "    ~~~~~~~~~~~^\n",
      "        _LIB.XGBoosterUpdateOneIter(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "            self.handle, ctypes.c_int(iteration), dtrain.handle\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        )\n",
      "        ^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [11:43:49] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\common\\io.h:320: bad_malloc: Failed to allocate 199977456 bytes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1108, in fit\n",
      "    self._Booster = train(\n",
      "                    ~~~~~^\n",
      "        params,\n",
      "        ^^^^^^^\n",
      "    ...<9 lines>...\n",
      "        callbacks=self.callbacks,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "    ~~~~~~~~~~~^\n",
      "        _LIB.XGBoosterUpdateOneIter(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "            self.handle, ctypes.c_int(iteration), dtrain.handle\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        )\n",
      "        ^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [11:43:54] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\common\\io.h:320: bad_malloc: Failed to allocate 159758400 bytes.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [-0.17714024 -0.17741485 -0.14199434 -0.14236221 -0.12096444 -0.12149549\n",
      " -0.11375558 -0.11452184 -0.15742029 -0.1579475  -0.12589437 -0.12653337\n",
      " -0.10970499 -0.11054938 -0.10405941 -0.10520478 -0.14372149 -0.14444086\n",
      " -0.11630493 -0.11722392 -0.10343915 -0.10471511 -0.0990865  -0.10072169\n",
      " -0.13560459 -0.13659499 -0.11173236 -0.11334332 -0.10145058         nan\n",
      " -0.09848238         nan -0.10816752 -0.10808335 -0.10151677 -0.10168468\n",
      " -0.09734965 -0.09723969 -0.09597442 -0.09553509 -0.10195912 -0.10170317\n",
      " -0.0980089  -0.09763179 -0.09631479 -0.09557237 -0.09609965 -0.09500106\n",
      " -0.10057799 -0.10017914 -0.09868033 -0.09807031 -0.09820268 -0.09726545\n",
      " -0.09812433 -0.09714241 -0.10288875 -0.10265414 -0.1021731  -0.10179634\n",
      " -0.10202968 -0.1015756  -0.10202158 -0.10155032 -0.10648977 -0.10490385\n",
      " -0.10543322 -0.10304318 -0.10693652 -0.10352758 -0.108636   -0.10443516\n",
      " -0.11042211 -0.10676846 -0.11221398 -0.10766181 -0.11382668 -0.10866652\n",
      " -0.1141589  -0.10887751 -0.11694307 -0.11203706 -0.11760269 -0.11234878\n",
      " -0.1176113  -0.11235951 -0.11761053 -0.11235684 -0.1220979  -0.11646246\n",
      " -0.12206772 -0.11643833 -0.12206693 -0.11643843 -0.1220668  -0.11643843]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost (tuned parameters):\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 1200, 'subsample': 1.0}\n",
      "Mean RMSE (CV): 0.308, Mean R-squared (CV): 0.805\n",
      "\n",
      "=== Feature Set 3 (Vader) ===\n",
      "Baseline (Mean Prediction):\n",
      "RMSE: 0.698, R-squared: 0.000\n",
      "\n",
      "Linear Regression:\n",
      "Mean RMSE (CV): 0.450, Mean R-squared (CV): 0.584\n",
      "\n",
      "XGBoost (default parameters):\n",
      "Mean RMSE (CV): 0.330, Mean R-squared (CV): 0.776\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      "XGBoost (tuned parameters):\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 1200, 'subsample': 1.0}\n",
      "Mean RMSE (CV): 0.309, Mean R-squared (CV): 0.805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Set Evaluation: Feature Set 3 (TextBlob) ===\n",
      "Test RMSE: 0.968, Test R-squared: -0.917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Set Evaluation: Feature Set 3 (Vader) ===\n",
      "Test RMSE: 1.072, Test R-squared: -1.352\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate a model with cross-validation\n",
    "def evaluate_model(model, X, y, cv):\n",
    "    rmse_scores, r2_scores = [], []\n",
    "    for train_idx, val_idx in cv.split(X):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_val, y_pred_val)))\n",
    "        r2_scores.append(r2_score(y_val, y_pred_val))\n",
    "    return np.mean(rmse_scores), np.mean(r2_scores)\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to evaluate a feature set\n",
    "def evaluate_feature_set(features_train, target_train, feature_set_name):\n",
    "    print(f\"\\n=== {feature_set_name} ===\")\n",
    "    \n",
    "    # Baseline 1: Mean Prediction\n",
    "    baseline_mean_price = target_train.mean()\n",
    "    baseline_rmse = np.sqrt(mean_squared_error(target_train, [baseline_mean_price] * len(target_train)))\n",
    "    baseline_r2 = r2_score(target_train, [baseline_mean_price] * len(target_train))\n",
    "    print(\"Baseline (Mean Prediction):\")\n",
    "    print(f\"RMSE: {baseline_rmse:.3f}, R-squared: {baseline_r2:.3f}\")\n",
    "    \n",
    "    # Baseline 2: Linear Regression\n",
    "    linear_model = LinearRegression()\n",
    "    rmse_lr, r2_lr = evaluate_model(linear_model, features_train, target_train, kf)\n",
    "    print(\"\\nLinear Regression:\")\n",
    "    print(f\"Mean RMSE (CV): {rmse_lr:.3f}, Mean R-squared (CV): {r2_lr:.3f}\")\n",
    "    \n",
    "    # XGBoost with default parameters\n",
    "    xgb_default = XGBRegressor(random_state=42)\n",
    "    rmse_xgb_default, r2_xgb_default = evaluate_model(xgb_default, features_train, target_train, kf)\n",
    "    print(\"\\nXGBoost (default parameters):\")\n",
    "    print(f\"Mean RMSE (CV): {rmse_xgb_default:.3f}, Mean R-squared (CV): {r2_xgb_default:.3f}\")\n",
    "    \n",
    "    # GridSearchCV for XGBoost\n",
    "    param_grid = {\n",
    "    'max_depth': [6, 8, 10, 12],            # Moderate tree depths\n",
    "    'n_estimators': [200, 400, 800, 1200],      # Number of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.3],    # Step size shrinkage\n",
    "    'subsample': [0.8, 1.0],              # Row sampling\n",
    "}\n",
    "    xgb_grid_search = GridSearchCV(\n",
    "        estimator=XGBRegressor(random_state=42),\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=kf,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    xgb_grid_search.fit(features_train, target_train)\n",
    "    \n",
    "    # Save best model and parameters\n",
    "    best_model = xgb_grid_search.best_estimator_\n",
    "    best_params = xgb_grid_search.best_params_\n",
    "    rmse_xgb_tuned, r2_xgb_tuned = evaluate_model(best_model, features_train, target_train, kf)\n",
    "    \n",
    "    print(\"\\nXGBoost (tuned parameters):\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Mean RMSE (CV): {rmse_xgb_tuned:.3f}, Mean R-squared (CV): {r2_xgb_tuned:.3f}\")\n",
    "    \n",
    "    return best_model, best_params\n",
    "\n",
    "# Prepare TextBlob data\n",
    "scaler_textblob = StandardScaler()\n",
    "features_train_textblob = scaler_textblob.fit_transform(feature_set_3_textblob_train.drop(columns=['price']))\n",
    "target_train_textblob = feature_set_3_textblob_train['price']\n",
    "\n",
    "# Evaluate TextBlob and save results\n",
    "best_xgb_textblob, best_params_textblob = evaluate_feature_set(features_train_textblob, target_train_textblob, \"Feature Set 3 (TextBlob)\")\n",
    "\n",
    "# Prepare Vader data\n",
    "scaler_vader = StandardScaler()\n",
    "features_train_vader = scaler_vader.fit_transform(feature_set_3_vader_train.drop(columns=['price']))\n",
    "target_train_vader = feature_set_3_vader_train['price']\n",
    "\n",
    "# Evaluate Vader and save results\n",
    "best_xgb_vader, best_params_vader = evaluate_feature_set(features_train_vader, target_train_vader, \"Feature Set 3 (Vader)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning comparing Textblob and Vader for feature set 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Set 4 (TextBlob) ===\n",
      "Baseline (Mean Prediction):\n",
      "RMSE: 0.698, R-squared: 0.000\n",
      "\n",
      "Linear Regression:\n",
      "Mean RMSE (CV): 0.669, Mean R-squared (CV): 0.082\n",
      "\n",
      "XGBoost (default parameters):\n",
      "Mean RMSE (CV): 0.633, Mean R-squared: 0.178\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      "XGBoost (tuned parameters):\n",
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 1200, 'subsample': 0.8}\n",
      "Mean RMSE (CV): 0.627, Mean R-squared (CV): 0.193\n",
      "\n",
      "=== Feature Set 4 (Vader) ===\n",
      "Baseline (Mean Prediction):\n",
      "RMSE: 0.698, R-squared: 0.000\n",
      "\n",
      "Linear Regression:\n",
      "Mean RMSE (CV): 0.673, Mean R-squared (CV): 0.071\n",
      "\n",
      "XGBoost (default parameters):\n",
      "Mean RMSE (CV): 0.641, Mean R-squared: 0.157\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      "XGBoost (tuned parameters):\n",
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 1200, 'subsample': 0.8}\n",
      "Mean RMSE (CV): 0.635, Mean R-squared (CV): 0.171\n",
      "\n",
      "=== Test Set Evaluation: Feature Set 4 (TextBlob) ===\n",
      "Test RMSE: 0.628, Test R-squared: 0.193\n",
      "\n",
      "=== Test Set Evaluation: Feature Set 4 (Vader) ===\n",
      "Test RMSE: 0.636, Test R-squared: 0.172\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate a model with cross-validation\n",
    "def evaluate_model(model, X, y, cv):\n",
    "    rmse_scores, r2_scores = [], []\n",
    "    for train_idx, val_idx in cv.split(X):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_val, y_pred_val)))\n",
    "        r2_scores.append(r2_score(y_val, y_pred_val))\n",
    "    return np.mean(rmse_scores), np.mean(r2_scores)\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to evaluate a feature set\n",
    "def evaluate_feature_set(features_train, target_train, feature_set_name):\n",
    "    print(f\"\\n=== {feature_set_name} ===\")\n",
    "    \n",
    "    # Baseline 1: Mean Prediction\n",
    "    baseline_mean_price = target_train.mean()\n",
    "    baseline_rmse = np.sqrt(mean_squared_error(target_train, [baseline_mean_price] * len(target_train)))\n",
    "    baseline_r2 = r2_score(target_train, [baseline_mean_price] * len(target_train))\n",
    "    print(\"Baseline (Mean Prediction):\")\n",
    "    print(f\"RMSE: {baseline_rmse:.3f}, R-squared: {baseline_r2:.3f}\")\n",
    "    \n",
    "    # Baseline 2: Linear Regression\n",
    "    linear_model = LinearRegression()\n",
    "    rmse_lr, r2_lr = evaluate_model(linear_model, features_train, target_train, kf)\n",
    "    print(\"\\nLinear Regression:\")\n",
    "    print(f\"Mean RMSE (CV): {rmse_lr:.3f}, Mean R-squared (CV): {r2_lr:.3f}\")\n",
    "    \n",
    "    # XGBoost with default parameters\n",
    "    xgb_default = XGBRegressor(random_state=42)\n",
    "    rmse_xgb_default, r2_xgb_default = evaluate_model(xgb_default, features_train, target_train, kf)\n",
    "    print(\"\\nXGBoost (default parameters):\")\n",
    "    print(f\"Mean RMSE (CV): {rmse_xgb_default:.3f}, Mean R-squared: {r2_xgb_default:.3f}\")\n",
    "    \n",
    "    # GridSearchCV for XGBoost\n",
    "    param_grid =  {\n",
    "    'max_depth': [6, 8, 10, 12],            # Moderate tree depths\n",
    "    'n_estimators': [200, 400, 800, 1200],      # Number of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.3],    # Step size shrinkage\n",
    "    'subsample': [0.8, 1.0],              # Row sampling\n",
    "}\n",
    "    \n",
    "    xgb_grid_search = GridSearchCV(\n",
    "        estimator=XGBRegressor(random_state=42),\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=kf,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    xgb_grid_search.fit(features_train, target_train)\n",
    "    \n",
    "    # Save best model and parameters\n",
    "    best_model = xgb_grid_search.best_estimator_\n",
    "    best_params = xgb_grid_search.best_params_\n",
    "    rmse_xgb_tuned, r2_xgb_tuned = evaluate_model(best_model, features_train, target_train, kf)\n",
    "    \n",
    "    print(\"\\nXGBoost (tuned parameters):\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Mean RMSE (CV): {rmse_xgb_tuned:.3f}, Mean R-squared (CV): {r2_xgb_tuned:.3f}\")\n",
    "    \n",
    "    return best_model, best_params\n",
    "\n",
    "# Prepare TextBlob data for Feature Set 4\n",
    "scaler_textblob = StandardScaler()\n",
    "features_train_textblob = scaler_textblob.fit_transform(feature_set_4_textblob_train.drop(columns=['price']))\n",
    "target_train_textblob = feature_set_4_textblob_train['price']\n",
    "\n",
    "# Evaluate TextBlob for Feature Set 4\n",
    "best_xgb_textblob_4, best_params_textblob_4 = evaluate_feature_set(features_train_textblob, target_train_textblob, \"Feature Set 4 (TextBlob)\")\n",
    "\n",
    "# Prepare Vader data for Feature Set 4\n",
    "scaler_vader = StandardScaler()\n",
    "features_train_vader = scaler_vader.fit_transform(feature_set_4_vader_train.drop(columns=['price']))\n",
    "target_train_vader = feature_set_4_vader_train['price']\n",
    "\n",
    "# Evaluate Vader for Feature Set 4\n",
    "best_xgb_vader_4, best_params_vader_4 = evaluate_feature_set(features_train_vader, target_train_vader, \"Feature Set 4 (Vader)\")\n",
    "\n",
    "# Test set evaluation\n",
    "def test_model(model, scaler, features_test, target_test, feature_set_name):\n",
    "    # Scale the test data using the same scaler as training\n",
    "    features_test_scaled = scaler.transform(features_test)\n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(features_test_scaled)\n",
    "    # Evaluate RMSE and R-squared\n",
    "    rmse_test = np.sqrt(mean_squared_error(target_test, y_pred_test))\n",
    "    r2_test = r2_score(target_test, y_pred_test)\n",
    "    print(f\"\\n=== Test Set Evaluation: {feature_set_name} ===\")\n",
    "    print(f\"Test RMSE: {rmse_test:.3f}, Test R-squared: {r2_test:.3f}\")\n",
    "\n",
    "# Test TextBlob for Feature Set 4\n",
    "features_test_textblob = feature_set_4_textblob_test.drop(columns=['price'])\n",
    "target_test_textblob = feature_set_4_textblob_test['price']\n",
    "test_model(best_xgb_textblob_4, scaler_textblob, features_test_textblob, target_test_textblob, \"Feature Set 4 (TextBlob)\")\n",
    "\n",
    "# Test Vader for Feature Set 4\n",
    "features_test_vader = feature_set_4_vader_test.drop(columns=['price'])\n",
    "target_test_vader = feature_set_4_vader_test['price']\n",
    "test_model(best_xgb_vader_4, scaler_vader, features_test_vader, target_test_vader, \"Feature Set 4 (Vader)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
